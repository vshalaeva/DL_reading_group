# Overview
Thransformer is a NN language model based on only self-attention mechanism. 
Most of language models based on RNN attention mechanisms. 
Tranformer allows us parallelize training by avoiding use of RNN.

Self-attention is an attention mechanism relating different positions of a single sequence in 
order to compute a representation of the sequence. Details of computations is nicely described in the blog post 
"Illustrated Transformer".

# References
1. Original paper [Vaswani_et_al Attention is all you need](https://arxiv.org/abs/1706.03762)
2. Blog post [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
3. [TalkToTransformer](https://talktotransformer.com)
